# Set Autoencoder

An autoencoder for sets of elements. 
While set encoders have existed for some time, (such as [Deep Sets](https://arxiv.org/abs/1703.06114)), 
there is currently no good method for decoding sets in prior literature. The difficulty of this problem
stems from the fact that sets have no ordering and a variable number of elements, while standard neural networks
produce a fixed number of outputs with a definite ordering. There are some networks which can produce a variable number
of outputs (like [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)), but they impose an ordering on the output.
The issue with networks which produce ordered outputs is that they recognise each ordering as a distinct output. Consequently,
the task of learning becomes combinatorialâ€“since the network can't generalise, it must be trained on each possible permutation
of the output.

The usefulness of a set decoder (and, by extension, a set autoencoder) is quite clear. To list just a few possible applications:
- classification of a variable number of objects in an image
- multiple task assignment given a variable number of tasks
- producing continuous, multi-modal action distributions with a sum of a variable number of gaussians (usually to produce a multi-modal distribution, it must be discrete)
- A building block for a graph autoencoder, which can be used for neural cellular automata

## Architecture

### Encoder
The encoder takes a set {x1, x2, ... xn} as an input. 

First, in order to maintain permutation invariance, the set is reordered according to some random ranking function. This is important because, while our aggregation function is permutation invariant, the act of assigning keys to values (*i.e.* assigning the value x1 to the key 1) implicitly introduces an ordering. However, by assigning keys according to some static function, we maintain permutation invariance.

Next, each key (in one hot format) passes through a key encoder to produce a query, and each corresponding element passes through a value encoder to produce a value. This brings the keys and elements to the same dimension as the latent space. The value encoder learns the best way to encode each element, and the key encoder learns how to "insert" those encodings into the latent space.

An elementwise multiplication with an added bias is performed between the queries and the values in order to produce encodings in the latent space. Then, a single, aggregated encoding is generated by summing over all n encodings.

Finally, the resulting encoding is concatenated with n (in a one-hot format). This information is combined in a final network to produce the output latent space.

Note that if the components of the encoder associated with keys and the size of the set are removed, then the architecture of our encoder is analagous to that of Deep Sets. This is a desirable property, because Deep Sets is a universal set function approximator.

![Encoder](https://github.com/Acciorocketships/SetAutoEncoder/blob/main/schema/encoder.png)


### Decoder
The decoder take in the fixed-size latent space as an input.

First, the decoder predicts the length of the set n, which was just encoded in the last element of the encoder. While it would also be possible to simply concatenate the true length of the set in the latent space, we use this method in order to avoid assuming a form on the latent space. This becomes useful in scenarios where the decoder is used by itself as a component in a neural network.

Next, keys from 1 to the predicted n are generated in one-hot format. Each key is concatenated with a copy of the entire latent space to produce an element-specific encoding.

Finally, all of the encodings pass through the decoder to produce the output set. While the outputs are unordered, a correspondence between the output and the input can be established by matching the keys (for example, the element associated with key 1 in the encoder corresponds to the element corresponding to key 1 in the decoder). 
![Decoder](https://github.com/Acciorocketships/SetAutoEncoder/blob/main/schema/decoder.png)
